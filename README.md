# Prompt Engineering for Secure Code

This repository accompanies our study **"Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models"**. The study investigates how various prompt engineering techniques influence the security of code generation.

## Repository Structure

- **`benchmark/`**: Contains the benchmarking tool and experimental results. The benchmark systematically evaluates prompt variations and their impact on secure code generation using static analysis.
- **`agent/`**: A prototype implementation of a prompt agent that applies our findings to improve code security in LLM code generation. It extends an existing ChatGPT web interface with a security-focused prompt prefix and optional post-processing.

## Preprint

For full details, methodology, and results, refer to our preprint:  
**[Link to Preprint]** (TBD)

## Citation

If you find this work useful, please consider citing our paper.